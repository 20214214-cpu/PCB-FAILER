digraph {
	graph [size="113.85,113.85"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2734001063696 [label="
 (1, 2)" fillcolor=darkolivegreen1]
	2733938699968 -> 2734001062832 [dir=none]
	2734001062832 [label="mat1
 (1, 512)" fillcolor=orange]
	2733938699968 -> 2734001063888 [dir=none]
	2734001063888 [label="mat2
 (512, 2)" fillcolor=orange]
	2733938699968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 512)
mat1_sym_strides:       (512, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (512, 2)
mat2_sym_strides:       (1, 512)"]
	2733938700976 -> 2733938699968
	2733936475792 [label="fc.bias
 (2)" fillcolor=lightblue]
	2733936475792 -> 2733938700976
	2733938700976 [label=AccumulateGrad]
	2733938700400 -> 2733938699968
	2733938700400 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 512, 1, 1)"]
	2733938701024 -> 2733938700400
	2733938701024 [label="MeanBackward1
----------------------------------------
dim           : (4294967295, 4294967294)
keepdim       :                     True
self_sym_numel:                    25088
self_sym_sizes:           (1, 512, 7, 7)"]
	2733938701312 -> 2733938701024
	2733938701312 -> 2734001064368 [dir=none]
	2734001064368 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2733938701312 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2733938701216 -> 2733938701312
	2733938701216 [label="AddBackward0
------------
alpha: 1"]
	2733938701120 -> 2733938701216
	2733938701120 -> 2734001062448 [dir=none]
	2734001062448 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2733938701120 -> 2734001065616 [dir=none]
	2734001065616 [label="result1
 (0)" fillcolor=orange]
	2733938701120 -> 2734001064848 [dir=none]
	2734001064848 [label="result2
 (0)" fillcolor=orange]
	2733938701120 -> 2733936464176 [dir=none]
	2733936464176 [label="running_mean
 (512)" fillcolor=orange]
	2733938701120 -> 2733936464560 [dir=none]
	2733936464560 [label="running_var
 (512)" fillcolor=orange]
	2733938701120 -> 2733936464368 [dir=none]
	2733936464368 [label="weight
 (512)" fillcolor=orange]
	2733938701120 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2733938701504 -> 2733938701120
	2733938701504 -> 2734001063312 [dir=none]
	2734001063312 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2733938701504 -> 2733936464272 [dir=none]
	2733936464272 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2733938701504 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2733938701696 -> 2733938701504
	2733938701696 -> 2734001066672 [dir=none]
	2734001066672 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2733938701696 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2733938701840 -> 2733938701696
	2733938701840 -> 2734001066096 [dir=none]
	2734001066096 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2733938701840 -> 2734001066960 [dir=none]
	2734001066960 [label="result1
 (0)" fillcolor=orange]
	2733938701840 -> 2734001067344 [dir=none]
	2734001067344 [label="result2
 (0)" fillcolor=orange]
	2733938701840 -> 2733936463600 [dir=none]
	2733936463600 [label="running_mean
 (512)" fillcolor=orange]
	2733938701840 -> 2733936463984 [dir=none]
	2733936463984 [label="running_var
 (512)" fillcolor=orange]
	2733938701840 -> 2733936463792 [dir=none]
	2733936463792 [label="weight
 (512)" fillcolor=orange]
	2733938701840 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2733938701936 -> 2733938701840
	2733938701936 -> 2734001065328 [dir=none]
	2734001065328 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2733938701936 -> 2733936463696 [dir=none]
	2733936463696 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2733938701936 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2733938701168 -> 2733938701936
	2733938701168 -> 2734001068112 [dir=none]
	2734001068112 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2733938701168 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2733938702224 -> 2733938701168
	2733938702224 [label="AddBackward0
------------
alpha: 1"]
	2733938702320 -> 2733938702224
	2733938702320 -> 2734001063600 [dir=none]
	2734001063600 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2733938702320 -> 2734001068592 [dir=none]
	2734001068592 [label="result1
 (0)" fillcolor=orange]
	2733938702320 -> 2734001068880 [dir=none]
	2734001068880 [label="result2
 (0)" fillcolor=orange]
	2733938702320 -> 2733936463024 [dir=none]
	2733936463024 [label="running_mean
 (512)" fillcolor=orange]
	2733938702320 -> 2733936463408 [dir=none]
	2733936463408 [label="running_var
 (512)" fillcolor=orange]
	2733938702320 -> 2733936463216 [dir=none]
	2733936463216 [label="weight
 (512)" fillcolor=orange]
	2733938702320 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2733938702464 -> 2733938702320
	2733938702464 -> 2734001064560 [dir=none]
	2734001064560 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2733938702464 -> 2733936463120 [dir=none]
	2733936463120 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2733938702464 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2733938702656 -> 2733938702464
	2733938702656 -> 2734001069648 [dir=none]
	2734001069648 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2733938702656 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2733938702800 -> 2733938702656
	2733938702800 -> 2734001061968 [dir=none]
	2734001061968 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2733938702800 -> 2734001069936 [dir=none]
	2734001069936 [label="result1
 (0)" fillcolor=orange]
	2733938702800 -> 2734001070320 [dir=none]
	2734001070320 [label="result2
 (0)" fillcolor=orange]
	2733938702800 -> 2733936462448 [dir=none]
	2733936462448 [label="running_mean
 (512)" fillcolor=orange]
	2733938702800 -> 2733936462832 [dir=none]
	2733936462832 [label="running_var
 (512)" fillcolor=orange]
	2733938702800 -> 2733936462640 [dir=none]
	2733936462640 [label="weight
 (512)" fillcolor=orange]
	2733938702800 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2733938702896 -> 2733938702800
	2733938702896 -> 2734001066000 [dir=none]
	2734001066000 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2733938702896 -> 2733936462544 [dir=none]
	2733936462544 [label="weight
 (512, 256, 3, 3)" fillcolor=orange]
	2733938702896 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2733938703088 -> 2733938702896
	2733938703088 -> 2734001071088 [dir=none]
	2734001071088 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2733938703088 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2733938703232 -> 2733938703088
	2733938703232 [label="AddBackward0
------------
alpha: 1"]
	2733938703328 -> 2733938703232
	2733938703328 -> 2734001065808 [dir=none]
	2734001065808 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2733938703328 -> 2734001071568 [dir=none]
	2734001071568 [label="result1
 (0)" fillcolor=orange]
	2733938703328 -> 2734001071856 [dir=none]
	2734001071856 [label="result2
 (0)" fillcolor=orange]
	2733938703328 -> 2733936461296 [dir=none]
	2733936461296 [label="running_mean
 (256)" fillcolor=orange]
	2733938703328 -> 2733936461680 [dir=none]
	2733936461680 [label="running_var
 (256)" fillcolor=orange]
	2733938703328 -> 2733936461488 [dir=none]
	2733936461488 [label="weight
 (256)" fillcolor=orange]
	2733938703328 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2733938703472 -> 2733938703328
	2733938703472 -> 2734000831952 [dir=none]
	2734000831952 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2733938703472 -> 2733936461392 [dir=none]
	2733936461392 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2733938703472 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2733938703664 -> 2733938703472
	2733938703664 -> 2734001072624 [dir=none]
	2734001072624 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2733938703664 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2733938703808 -> 2733938703664
	2733938703808 -> 2734000821008 [dir=none]
	2734000821008 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2733938703808 -> 2734001072912 [dir=none]
	2734001072912 [label="result1
 (0)" fillcolor=orange]
	2733938703808 -> 2734001073296 [dir=none]
	2734001073296 [label="result2
 (0)" fillcolor=orange]
	2733938703808 -> 2733936460720 [dir=none]
	2733936460720 [label="running_mean
 (256)" fillcolor=orange]
	2733938703808 -> 2733936461104 [dir=none]
	2733936461104 [label="running_var
 (256)" fillcolor=orange]
	2733938703808 -> 2733936460912 [dir=none]
	2733936460912 [label="weight
 (256)" fillcolor=orange]
	2733938703808 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2733938703904 -> 2733938703808
	2733938703904 -> 2734000823792 [dir=none]
	2734000823792 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2733938703904 -> 2733936460816 [dir=none]
	2733936460816 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2733938703904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2733938703280 -> 2733938703904
	2733938703280 -> 2734001074064 [dir=none]
	2734001074064 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2733938703280 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2733938704192 -> 2733938703280
	2733938704192 [label="AddBackward0
------------
alpha: 1"]
	2733938704288 -> 2733938704192
	2733938704288 -> 2734000827920 [dir=none]
	2734000827920 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2733938704288 -> 2734001074544 [dir=none]
	2734001074544 [label="result1
 (0)" fillcolor=orange]
	2733938704288 -> 2734001074832 [dir=none]
	2734001074832 [label="result2
 (0)" fillcolor=orange]
	2733938704288 -> 2733936460144 [dir=none]
	2733936460144 [label="running_mean
 (256)" fillcolor=orange]
	2733938704288 -> 2733936460528 [dir=none]
	2733936460528 [label="running_var
 (256)" fillcolor=orange]
	2733938704288 -> 2733936460336 [dir=none]
	2733936460336 [label="weight
 (256)" fillcolor=orange]
	2733938704288 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2733938704336 -> 2733938704288
	2733938704336 -> 2734000823504 [dir=none]
	2734000823504 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2733938704336 -> 2733936460240 [dir=none]
	2733936460240 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2733938704336 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2734001111344 -> 2733938704336
	2734001111344 -> 2734001075600 [dir=none]
	2734001075600 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2734001111344 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2734001111488 -> 2734001111344
	2734001111488 -> 2734000831280 [dir=none]
	2734000831280 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2734001111488 -> 2734001075888 [dir=none]
	2734001075888 [label="result1
 (0)" fillcolor=orange]
	2734001111488 -> 2734001076272 [dir=none]
	2734001076272 [label="result2
 (0)" fillcolor=orange]
	2734001111488 -> 2733936328432 [dir=none]
	2733936328432 [label="running_mean
 (256)" fillcolor=orange]
	2734001111488 -> 2733936459952 [dir=none]
	2733936459952 [label="running_var
 (256)" fillcolor=orange]
	2734001111488 -> 2733936328624 [dir=none]
	2733936328624 [label="weight
 (256)" fillcolor=orange]
	2734001111488 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2734001111584 -> 2734001111488
	2734001111584 -> 2734000828592 [dir=none]
	2734000828592 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2734001111584 -> 2733936328528 [dir=none]
	2733936328528 [label="weight
 (256, 128, 3, 3)" fillcolor=orange]
	2734001111584 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2734001111776 -> 2734001111584
	2734001111776 -> 2734001077040 [dir=none]
	2734001077040 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2734001111776 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2734001111920 -> 2734001111776
	2734001111920 [label="AddBackward0
------------
alpha: 1"]
	2734001112016 -> 2734001111920
	2734001112016 -> 2734000930448 [dir=none]
	2734000930448 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2734001112016 -> 2734001077520 [dir=none]
	2734001077520 [label="result1
 (0)" fillcolor=orange]
	2734001112016 -> 2734001077808 [dir=none]
	2734001077808 [label="result2
 (0)" fillcolor=orange]
	2734001112016 -> 2733936327280 [dir=none]
	2733936327280 [label="running_mean
 (128)" fillcolor=orange]
	2734001112016 -> 2733936327664 [dir=none]
	2733936327664 [label="running_var
 (128)" fillcolor=orange]
	2734001112016 -> 2733936327472 [dir=none]
	2733936327472 [label="weight
 (128)" fillcolor=orange]
	2734001112016 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2734001112160 -> 2734001112016
	2734001112160 -> 2734000929968 [dir=none]
	2734000929968 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2734001112160 -> 2733936327376 [dir=none]
	2733936327376 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2734001112160 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2734001112352 -> 2734001112160
	2734001112352 -> 2734001144176 [dir=none]
	2734001144176 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2734001112352 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2733938695600 -> 2734001112352
	2733938695600 -> 2734000923536 [dir=none]
	2734000923536 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2733938695600 -> 2734000921520 [dir=none]
	2734000921520 [label="result1
 (0)" fillcolor=orange]
	2733938695600 -> 2734001144464 [dir=none]
	2734001144464 [label="result2
 (0)" fillcolor=orange]
	2733938695600 -> 2733936326704 [dir=none]
	2733936326704 [label="running_mean
 (128)" fillcolor=orange]
	2733938695600 -> 2733936327088 [dir=none]
	2733936327088 [label="running_var
 (128)" fillcolor=orange]
	2733938695600 -> 2733936326896 [dir=none]
	2733936326896 [label="weight
 (128)" fillcolor=orange]
	2733938695600 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2733938698240 -> 2733938695600
	2733938698240 -> 2734000925552 [dir=none]
	2734000925552 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2733938698240 -> 2733936326800 [dir=none]
	2733936326800 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2733938698240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2734001111968 -> 2733938698240
	2734001111968 -> 2734001145232 [dir=none]
	2734001145232 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2734001111968 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2734001112688 -> 2734001111968
	2734001112688 [label="AddBackward0
------------
alpha: 1"]
	2734001112784 -> 2734001112688
	2734001112784 -> 2734000925072 [dir=none]
	2734000925072 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2734001112784 -> 2734001145712 [dir=none]
	2734001145712 [label="result1
 (0)" fillcolor=orange]
	2734001112784 -> 2734001146000 [dir=none]
	2734001146000 [label="result2
 (0)" fillcolor=orange]
	2734001112784 -> 2733936326128 [dir=none]
	2733936326128 [label="running_mean
 (128)" fillcolor=orange]
	2734001112784 -> 2733936326512 [dir=none]
	2733936326512 [label="running_var
 (128)" fillcolor=orange]
	2734001112784 -> 2733936326320 [dir=none]
	2733936326320 [label="weight
 (128)" fillcolor=orange]
	2734001112784 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2734001112928 -> 2734001112784
	2734001112928 -> 2734000922864 [dir=none]
	2734000922864 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2734001112928 -> 2733936326224 [dir=none]
	2733936326224 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2734001112928 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2734001113120 -> 2734001112928
	2734001113120 -> 2734001146768 [dir=none]
	2734001146768 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2734001113120 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2734001113264 -> 2734001113120
	2734001113264 -> 2734000927568 [dir=none]
	2734000927568 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2734001113264 -> 2734001147056 [dir=none]
	2734001147056 [label="result1
 (0)" fillcolor=orange]
	2734001113264 -> 2734001147440 [dir=none]
	2734001147440 [label="result2
 (0)" fillcolor=orange]
	2734001113264 -> 2733936325552 [dir=none]
	2733936325552 [label="running_mean
 (128)" fillcolor=orange]
	2734001113264 -> 2733936325936 [dir=none]
	2733936325936 [label="running_var
 (128)" fillcolor=orange]
	2734001113264 -> 2733936325744 [dir=none]
	2733936325744 [label="weight
 (128)" fillcolor=orange]
	2734001113264 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2734001113360 -> 2734001113264
	2734001113360 -> 2734000923344 [dir=none]
	2734000923344 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2734001113360 -> 2733936325648 [dir=none]
	2733936325648 [label="weight
 (128, 64, 3, 3)" fillcolor=orange]
	2734001113360 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2734001113552 -> 2734001113360
	2734001113552 -> 2734001148208 [dir=none]
	2734001148208 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2734001113552 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2734001113696 -> 2734001113552
	2734001113696 [label="AddBackward0
------------
alpha: 1"]
	2734001113792 -> 2734001113696
	2734001113792 -> 2734000926992 [dir=none]
	2734000926992 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2734001113792 -> 2734001148688 [dir=none]
	2734001148688 [label="result1
 (0)" fillcolor=orange]
	2734001113792 -> 2734001148976 [dir=none]
	2734001148976 [label="result2
 (0)" fillcolor=orange]
	2734001113792 -> 2733936324400 [dir=none]
	2733936324400 [label="running_mean
 (64)" fillcolor=orange]
	2734001113792 -> 2733936324784 [dir=none]
	2733936324784 [label="running_var
 (64)" fillcolor=orange]
	2734001113792 -> 2733936324592 [dir=none]
	2733936324592 [label="weight
 (64)" fillcolor=orange]
	2734001113792 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2734001113936 -> 2734001113792
	2734001113936 -> 2734000923248 [dir=none]
	2734000923248 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2734001113936 -> 2733936324496 [dir=none]
	2733936324496 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2734001113936 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2734001114128 -> 2734001113936
	2734001114128 -> 2734001149744 [dir=none]
	2734001149744 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2734001114128 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2734001114272 -> 2734001114128
	2734001114272 -> 2734000926896 [dir=none]
	2734000926896 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2734001114272 -> 2734001150032 [dir=none]
	2734001150032 [label="result1
 (0)" fillcolor=orange]
	2734001114272 -> 2734001150416 [dir=none]
	2734001150416 [label="result2
 (0)" fillcolor=orange]
	2734001114272 -> 2733936323824 [dir=none]
	2733936323824 [label="running_mean
 (64)" fillcolor=orange]
	2734001114272 -> 2733936324208 [dir=none]
	2733936324208 [label="running_var
 (64)" fillcolor=orange]
	2734001114272 -> 2733936324016 [dir=none]
	2733936324016 [label="weight
 (64)" fillcolor=orange]
	2734001114272 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2734001114368 -> 2734001114272
	2734001114368 -> 2734000924784 [dir=none]
	2734000924784 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2734001114368 -> 2733936323920 [dir=none]
	2733936323920 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2734001114368 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2734001113744 -> 2734001114368
	2734001113744 -> 2734001151184 [dir=none]
	2734001151184 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2734001113744 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2734001114656 -> 2734001113744
	2734001114656 [label="AddBackward0
------------
alpha: 1"]
	2734001114752 -> 2734001114656
	2734001114752 -> 2734000925936 [dir=none]
	2734000925936 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2734001114752 -> 2734001151664 [dir=none]
	2734001151664 [label="result1
 (0)" fillcolor=orange]
	2734001114752 -> 2734001151952 [dir=none]
	2734001151952 [label="result2
 (0)" fillcolor=orange]
	2734001114752 -> 2733936323248 [dir=none]
	2733936323248 [label="running_mean
 (64)" fillcolor=orange]
	2734001114752 -> 2733936323632 [dir=none]
	2733936323632 [label="running_var
 (64)" fillcolor=orange]
	2734001114752 -> 2733936323440 [dir=none]
	2733936323440 [label="weight
 (64)" fillcolor=orange]
	2734001114752 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2734001114896 -> 2734001114752
	2734001114896 -> 2734000925456 [dir=none]
	2734000925456 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2734001114896 -> 2733936323344 [dir=none]
	2733936323344 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2734001114896 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2734001115088 -> 2734001114896
	2734001115088 -> 2734001152720 [dir=none]
	2734001152720 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2734001115088 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2734001115232 -> 2734001115088
	2734001115232 -> 2734000924304 [dir=none]
	2734000924304 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2734001115232 -> 2734001153008 [dir=none]
	2734001153008 [label="result1
 (0)" fillcolor=orange]
	2734001115232 -> 2734001153392 [dir=none]
	2734001153392 [label="result2
 (0)" fillcolor=orange]
	2734001115232 -> 2733936322672 [dir=none]
	2733936322672 [label="running_mean
 (64)" fillcolor=orange]
	2734001115232 -> 2733936323056 [dir=none]
	2733936323056 [label="running_var
 (64)" fillcolor=orange]
	2734001115232 -> 2733936322864 [dir=none]
	2733936322864 [label="weight
 (64)" fillcolor=orange]
	2734001115232 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2734001115328 -> 2734001115232
	2734001115328 -> 2734000922960 [dir=none]
	2734000922960 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2734001115328 -> 2733936322768 [dir=none]
	2733936322768 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2734001115328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2734001114704 -> 2734001115328
	2734001114704 -> 2734001154160 [dir=none]
	2734001154160 [label="result1
 (1, 64, 56, 56)" fillcolor=orange]
	2734001114704 -> 2734000924976 [dir=none]
	2734000924976 [label="self
 (1, 64, 112, 112)" fillcolor=orange]
	2734001114704 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:         (3, 3)
padding    :         (1, 1)
result1    : [saved tensor]
self       : [saved tensor]
stride     :         (2, 2)"]
	2734001115616 -> 2734001114704
	2734001115616 -> 2734001154544 [dir=none]
	2734001154544 [label="result
 (1, 64, 112, 112)" fillcolor=orange]
	2734001115616 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2734001115712 -> 2734001115616
	2734001115712 -> 2734000930256 [dir=none]
	2734000930256 [label="input
 (1, 64, 112, 112)" fillcolor=orange]
	2734001115712 -> 2734001154832 [dir=none]
	2734001154832 [label="result1
 (0)" fillcolor=orange]
	2734001115712 -> 2734001155216 [dir=none]
	2734001155216 [label="result2
 (0)" fillcolor=orange]
	2734001115712 -> 2733936318544 [dir=none]
	2733936318544 [label="running_mean
 (64)" fillcolor=orange]
	2734001115712 -> 2733936322480 [dir=none]
	2733936322480 [label="running_var
 (64)" fillcolor=orange]
	2734001115712 -> 2733936322288 [dir=none]
	2733936322288 [label="weight
 (64)" fillcolor=orange]
	2734001115712 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2734001115808 -> 2734001115712
	2734001115808 -> 2734000927760 [dir=none]
	2734000927760 [label="input
 (1, 3, 224, 224)" fillcolor=orange]
	2734001115808 -> 2733936322192 [dir=none]
	2733936322192 [label="weight
 (64, 3, 7, 7)" fillcolor=orange]
	2734001115808 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2734001116000 -> 2734001115808
	2733936322192 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2733936322192 -> 2734001116000
	2734001116000 [label=AccumulateGrad]
	2734001115760 -> 2734001115712
	2733936322288 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2733936322288 -> 2734001115760
	2734001115760 [label=AccumulateGrad]
	2734001115424 -> 2734001115712
	2733936322384 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2733936322384 -> 2734001115424
	2734001115424 [label=AccumulateGrad]
	2734001115520 -> 2734001115328
	2733936322768 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2733936322768 -> 2734001115520
	2734001115520 [label=AccumulateGrad]
	2734001115280 -> 2734001115232
	2733936322864 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2733936322864 -> 2734001115280
	2734001115280 [label=AccumulateGrad]
	2734001115136 -> 2734001115232
	2733936322960 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2733936322960 -> 2734001115136
	2734001115136 [label=AccumulateGrad]
	2734001115040 -> 2734001114896
	2733936323344 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2733936323344 -> 2734001115040
	2734001115040 [label=AccumulateGrad]
	2734001114848 -> 2734001114752
	2733936323440 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2733936323440 -> 2734001114848
	2734001114848 [label=AccumulateGrad]
	2734001114800 -> 2734001114752
	2733936323536 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2733936323536 -> 2734001114800
	2734001114800 [label=AccumulateGrad]
	2734001114704 -> 2734001114656
	2734001114560 -> 2734001114368
	2733936323920 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2733936323920 -> 2734001114560
	2734001114560 [label=AccumulateGrad]
	2734001114320 -> 2734001114272
	2733936324016 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2733936324016 -> 2734001114320
	2734001114320 [label=AccumulateGrad]
	2734001114176 -> 2734001114272
	2733936324112 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2733936324112 -> 2734001114176
	2734001114176 [label=AccumulateGrad]
	2734001114080 -> 2734001113936
	2733936324496 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2733936324496 -> 2734001114080
	2734001114080 [label=AccumulateGrad]
	2734001113888 -> 2734001113792
	2733936324592 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2733936324592 -> 2734001113888
	2734001113888 [label=AccumulateGrad]
	2734001113840 -> 2734001113792
	2733936324688 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2733936324688 -> 2734001113840
	2734001113840 [label=AccumulateGrad]
	2734001113744 -> 2734001113696
	2734001113504 -> 2734001113360
	2733936325648 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2733936325648 -> 2734001113504
	2734001113504 [label=AccumulateGrad]
	2734001113312 -> 2734001113264
	2733936325744 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2733936325744 -> 2734001113312
	2734001113312 [label=AccumulateGrad]
	2734001113168 -> 2734001113264
	2733936325840 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2733936325840 -> 2734001113168
	2734001113168 [label=AccumulateGrad]
	2734001113072 -> 2734001112928
	2733936326224 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2733936326224 -> 2734001113072
	2734001113072 [label=AccumulateGrad]
	2734001112880 -> 2734001112784
	2733936326320 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2733936326320 -> 2734001112880
	2734001112880 [label=AccumulateGrad]
	2734001112832 -> 2734001112784
	2733936326416 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2733936326416 -> 2734001112832
	2734001112832 [label=AccumulateGrad]
	2734001112736 -> 2734001112688
	2734001112736 -> 2734000930352 [dir=none]
	2734000930352 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2734001112736 -> 2734001162384 [dir=none]
	2734001162384 [label="result1
 (0)" fillcolor=orange]
	2734001112736 -> 2734001162672 [dir=none]
	2734001162672 [label="result2
 (0)" fillcolor=orange]
	2734001112736 -> 2733936324976 [dir=none]
	2733936324976 [label="running_mean
 (128)" fillcolor=orange]
	2734001112736 -> 2733936325360 [dir=none]
	2733936325360 [label="running_var
 (128)" fillcolor=orange]
	2734001112736 -> 2733936325168 [dir=none]
	2733936325168 [label="weight
 (128)" fillcolor=orange]
	2734001112736 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2734001113456 -> 2734001112736
	2734001113456 -> 2734000923344 [dir=none]
	2734000923344 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2734001113456 -> 2733936325072 [dir=none]
	2733936325072 [label="weight
 (128, 64, 1, 1)" fillcolor=orange]
	2734001113456 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2734001113552 -> 2734001113456
	2734001113600 -> 2734001113456
	2733936325072 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2733936325072 -> 2734001113600
	2734001113600 [label=AccumulateGrad]
	2734001113024 -> 2734001112736
	2733936325168 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2733936325168 -> 2734001113024
	2734001113024 [label=AccumulateGrad]
	2734001112976 -> 2734001112736
	2733936325264 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2733936325264 -> 2734001112976
	2734001112976 [label=AccumulateGrad]
	2734001112592 -> 2733938698240
	2733936326800 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2733936326800 -> 2734001112592
	2734001112592 [label=AccumulateGrad]
	2733938698192 -> 2733938695600
	2733936326896 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2733936326896 -> 2733938698192
	2733938698192 [label=AccumulateGrad]
	2734001112496 -> 2733938695600
	2733936326992 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2733936326992 -> 2734001112496
	2734001112496 [label=AccumulateGrad]
	2734001112304 -> 2734001112160
	2733936327376 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2733936327376 -> 2734001112304
	2734001112304 [label=AccumulateGrad]
	2734001112112 -> 2734001112016
	2733936327472 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2733936327472 -> 2734001112112
	2734001112112 [label=AccumulateGrad]
	2734001112064 -> 2734001112016
	2733936327568 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2733936327568 -> 2734001112064
	2734001112064 [label=AccumulateGrad]
	2734001111968 -> 2734001111920
	2734001111728 -> 2734001111584
	2733936328528 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2733936328528 -> 2734001111728
	2734001111728 [label=AccumulateGrad]
	2734001111536 -> 2734001111488
	2733936328624 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2733936328624 -> 2734001111536
	2734001111536 [label=AccumulateGrad]
	2734001111392 -> 2734001111488
	2733936459856 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2733936459856 -> 2734001111392
	2734001111392 [label=AccumulateGrad]
	2734001111296 -> 2733938704336
	2733936460240 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2733936460240 -> 2734001111296
	2734001111296 [label=AccumulateGrad]
	2734001111152 -> 2733938704288
	2733936460336 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2733936460336 -> 2734001111152
	2734001111152 [label=AccumulateGrad]
	2734001111104 -> 2733938704288
	2733936460432 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2733936460432 -> 2734001111104
	2734001111104 [label=AccumulateGrad]
	2733938704240 -> 2733938704192
	2733938704240 -> 2734000831856 [dir=none]
	2734000831856 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2733938704240 -> 2734001168048 [dir=none]
	2734001168048 [label="result1
 (0)" fillcolor=orange]
	2733938704240 -> 2734001168336 [dir=none]
	2734001168336 [label="result2
 (0)" fillcolor=orange]
	2733938704240 -> 2733936327856 [dir=none]
	2733936327856 [label="running_mean
 (256)" fillcolor=orange]
	2733938704240 -> 2733936328240 [dir=none]
	2733936328240 [label="running_var
 (256)" fillcolor=orange]
	2733938704240 -> 2733936328048 [dir=none]
	2733936328048 [label="weight
 (256)" fillcolor=orange]
	2733938704240 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2733938697664 -> 2733938704240
	2733938697664 -> 2734000828592 [dir=none]
	2734000828592 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2733938697664 -> 2733936327952 [dir=none]
	2733936327952 [label="weight
 (256, 128, 1, 1)" fillcolor=orange]
	2733938697664 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2734001111776 -> 2733938697664
	2734001111824 -> 2733938697664
	2733936327952 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2733936327952 -> 2734001111824
	2734001111824 [label=AccumulateGrad]
	2734001111248 -> 2733938704240
	2733936328048 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2733936328048 -> 2734001111248
	2734001111248 [label=AccumulateGrad]
	2734001111200 -> 2733938704240
	2733936328144 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2733936328144 -> 2734001111200
	2734001111200 [label=AccumulateGrad]
	2733938704096 -> 2733938703904
	2733936460816 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2733936460816 -> 2733938704096
	2733938704096 [label=AccumulateGrad]
	2733938703856 -> 2733938703808
	2733936460912 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2733936460912 -> 2733938703856
	2733938703856 [label=AccumulateGrad]
	2733938703712 -> 2733938703808
	2733936461008 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2733936461008 -> 2733938703712
	2733938703712 [label=AccumulateGrad]
	2733938703616 -> 2733938703472
	2733936461392 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2733936461392 -> 2733938703616
	2733938703616 [label=AccumulateGrad]
	2733938703424 -> 2733938703328
	2733936461488 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2733936461488 -> 2733938703424
	2733938703424 [label=AccumulateGrad]
	2733938703376 -> 2733938703328
	2733936461584 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2733936461584 -> 2733938703376
	2733938703376 [label=AccumulateGrad]
	2733938703280 -> 2733938703232
	2733938703040 -> 2733938702896
	2733936462544 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2733936462544 -> 2733938703040
	2733938703040 [label=AccumulateGrad]
	2733938702848 -> 2733938702800
	2733936462640 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2733936462640 -> 2733938702848
	2733938702848 [label=AccumulateGrad]
	2733938702704 -> 2733938702800
	2733936462736 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2733936462736 -> 2733938702704
	2733938702704 [label=AccumulateGrad]
	2733938702608 -> 2733938702464
	2733936463120 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2733936463120 -> 2733938702608
	2733938702608 [label=AccumulateGrad]
	2733938702416 -> 2733938702320
	2733936463216 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2733936463216 -> 2733938702416
	2733938702416 [label=AccumulateGrad]
	2733938702368 -> 2733938702320
	2733936463312 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2733936463312 -> 2733938702368
	2733938702368 [label=AccumulateGrad]
	2733938702272 -> 2733938702224
	2733938702272 -> 2734001063216 [dir=none]
	2734001063216 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2733938702272 -> 2734001173712 [dir=none]
	2734001173712 [label="result1
 (0)" fillcolor=orange]
	2733938702272 -> 2734001174000 [dir=none]
	2734001174000 [label="result2
 (0)" fillcolor=orange]
	2733938702272 -> 2733936461872 [dir=none]
	2733936461872 [label="running_mean
 (512)" fillcolor=orange]
	2733938702272 -> 2733936462256 [dir=none]
	2733936462256 [label="running_var
 (512)" fillcolor=orange]
	2733938702272 -> 2733936462064 [dir=none]
	2733936462064 [label="weight
 (512)" fillcolor=orange]
	2733938702272 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2733938702992 -> 2733938702272
	2733938702992 -> 2734001066000 [dir=none]
	2734001066000 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2733938702992 -> 2733936461968 [dir=none]
	2733936461968 [label="weight
 (512, 256, 1, 1)" fillcolor=orange]
	2733938702992 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2733938703088 -> 2733938702992
	2733938703136 -> 2733938702992
	2733936461968 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2733936461968 -> 2733938703136
	2733938703136 [label=AccumulateGrad]
	2733938702560 -> 2733938702272
	2733936462064 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2733936462064 -> 2733938702560
	2733938702560 [label=AccumulateGrad]
	2733938702512 -> 2733938702272
	2733936462160 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2733936462160 -> 2733938702512
	2733938702512 [label=AccumulateGrad]
	2733938702128 -> 2733938701936
	2733936463696 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2733936463696 -> 2733938702128
	2733938702128 [label=AccumulateGrad]
	2733938701888 -> 2733938701840
	2733936463792 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2733936463792 -> 2733938701888
	2733938701888 [label=AccumulateGrad]
	2733938701744 -> 2733938701840
	2733936463888 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2733936463888 -> 2733938701744
	2733938701744 [label=AccumulateGrad]
	2733938701648 -> 2733938701504
	2733936464272 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2733936464272 -> 2733938701648
	2733938701648 [label=AccumulateGrad]
	2733938701456 -> 2733938701120
	2733936464368 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2733936464368 -> 2733938701456
	2733938701456 [label=AccumulateGrad]
	2733938701408 -> 2733938701120
	2733936464464 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2733936464464 -> 2733938701408
	2733938701408 [label=AccumulateGrad]
	2733938701168 -> 2733938701216
	2733938700064 -> 2733938699968
	2733938700064 [label=TBackward0]
	2733938701264 -> 2733938700064
	2733936475984 [label="fc.weight
 (2, 512)" fillcolor=lightblue]
	2733936475984 -> 2733938701264
	2733938701264 [label=AccumulateGrad]
	2733938699968 -> 2734001063696
}
