digraph {
	graph [size="113.85,113.85"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2670865671248 [label="
 (1, 2)" fillcolor=darkolivegreen1]
	2670811687776 -> 2670865666928 [dir=none]
	2670865666928 [label="mat1
 (1, 512)" fillcolor=orange]
	2670811687776 -> 2670865672112 [dir=none]
	2670865672112 [label="mat2
 (512, 2)" fillcolor=orange]
	2670811687776 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 512)
mat1_sym_strides:       (512, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (512, 2)
mat2_sym_strides:       (1, 512)"]
	2670811688304 -> 2670811687776
	2670809366160 [label="fc.bias
 (2)" fillcolor=lightblue]
	2670809366160 -> 2670811688304
	2670811688304 [label=AccumulateGrad]
	2670811687536 -> 2670811687776
	2670811687536 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 512, 1, 1)"]
	2670811688688 -> 2670811687536
	2670811688688 [label="MeanBackward1
----------------------------------------
dim           : (4294967295, 4294967294)
keepdim       :                     True
self_sym_numel:                    25088
self_sym_sizes:           (1, 512, 7, 7)"]
	2670811688976 -> 2670811688688
	2670811688976 -> 2670865669136 [dir=none]
	2670865669136 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2670811688976 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670811688880 -> 2670811688976
	2670811688880 [label="AddBackward0
------------
alpha: 1"]
	2670811688784 -> 2670811688880
	2670811688784 -> 2670865670960 [dir=none]
	2670865670960 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2670811688784 -> 2670865673264 [dir=none]
	2670865673264 [label="result1
 (0)" fillcolor=orange]
	2670811688784 -> 2670865673552 [dir=none]
	2670865673552 [label="result2
 (0)" fillcolor=orange]
	2670811688784 -> 2670809359152 [dir=none]
	2670809359152 [label="running_mean
 (512)" fillcolor=orange]
	2670811688784 -> 2670809359536 [dir=none]
	2670809359536 [label="running_var
 (512)" fillcolor=orange]
	2670811688784 -> 2670809359344 [dir=none]
	2670809359344 [label="weight
 (512)" fillcolor=orange]
	2670811688784 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670811689168 -> 2670811688784
	2670811689168 -> 2670865670864 [dir=none]
	2670865670864 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2670811689168 -> 2670809359248 [dir=none]
	2670809359248 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2670811689168 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670811689360 -> 2670811689168
	2670811689360 -> 2670865674320 [dir=none]
	2670865674320 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2670811689360 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670811689504 -> 2670811689360
	2670811689504 -> 2670865668080 [dir=none]
	2670865668080 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2670811689504 -> 2670865674608 [dir=none]
	2670865674608 [label="result1
 (0)" fillcolor=orange]
	2670811689504 -> 2670865674992 [dir=none]
	2670865674992 [label="result2
 (0)" fillcolor=orange]
	2670811689504 -> 2670809358576 [dir=none]
	2670809358576 [label="running_mean
 (512)" fillcolor=orange]
	2670811689504 -> 2670809358960 [dir=none]
	2670809358960 [label="running_var
 (512)" fillcolor=orange]
	2670811689504 -> 2670809358768 [dir=none]
	2670809358768 [label="weight
 (512)" fillcolor=orange]
	2670811689504 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670811689600 -> 2670811689504
	2670811689600 -> 2670865669616 [dir=none]
	2670865669616 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2670811689600 -> 2670809358672 [dir=none]
	2670809358672 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2670811689600 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670811688832 -> 2670811689600
	2670811688832 -> 2670865675760 [dir=none]
	2670865675760 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2670811688832 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670811689888 -> 2670811688832
	2670811689888 [label="AddBackward0
------------
alpha: 1"]
	2670811689984 -> 2670811689888
	2670811689984 -> 2670865667408 [dir=none]
	2670865667408 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2670811689984 -> 2670865676240 [dir=none]
	2670865676240 [label="result1
 (0)" fillcolor=orange]
	2670811689984 -> 2670865676528 [dir=none]
	2670865676528 [label="result2
 (0)" fillcolor=orange]
	2670811689984 -> 2670809358000 [dir=none]
	2670809358000 [label="running_mean
 (512)" fillcolor=orange]
	2670811689984 -> 2670809358384 [dir=none]
	2670809358384 [label="running_var
 (512)" fillcolor=orange]
	2670811689984 -> 2670809358192 [dir=none]
	2670809358192 [label="weight
 (512)" fillcolor=orange]
	2670811689984 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670811690128 -> 2670811689984
	2670811690128 -> 2670865668464 [dir=none]
	2670865668464 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2670811690128 -> 2670809358096 [dir=none]
	2670809358096 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2670811690128 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670811690320 -> 2670811690128
	2670811690320 -> 2670865677296 [dir=none]
	2670865677296 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2670811690320 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670811690464 -> 2670811690320
	2670811690464 -> 2670865667888 [dir=none]
	2670865667888 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2670811690464 -> 2670865677584 [dir=none]
	2670865677584 [label="result1
 (0)" fillcolor=orange]
	2670811690464 -> 2670865677968 [dir=none]
	2670865677968 [label="result2
 (0)" fillcolor=orange]
	2670811690464 -> 2670809357424 [dir=none]
	2670809357424 [label="running_mean
 (512)" fillcolor=orange]
	2670811690464 -> 2670809357808 [dir=none]
	2670809357808 [label="running_var
 (512)" fillcolor=orange]
	2670811690464 -> 2670809357616 [dir=none]
	2670809357616 [label="weight
 (512)" fillcolor=orange]
	2670811690464 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670811690560 -> 2670811690464
	2670811690560 -> 2670865669808 [dir=none]
	2670865669808 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2670811690560 -> 2670809357520 [dir=none]
	2670809357520 [label="weight
 (512, 256, 3, 3)" fillcolor=orange]
	2670811690560 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2670811690752 -> 2670811690560
	2670811690752 -> 2670865711568 [dir=none]
	2670865711568 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2670811690752 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670811690896 -> 2670811690752
	2670811690896 [label="AddBackward0
------------
alpha: 1"]
	2670811690992 -> 2670811690896
	2670811690992 -> 2670865668944 [dir=none]
	2670865668944 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2670811690992 -> 2670865712048 [dir=none]
	2670865712048 [label="result1
 (0)" fillcolor=orange]
	2670811690992 -> 2670865712336 [dir=none]
	2670865712336 [label="result2
 (0)" fillcolor=orange]
	2670811690992 -> 2670809356272 [dir=none]
	2670809356272 [label="running_mean
 (256)" fillcolor=orange]
	2670811690992 -> 2670809356656 [dir=none]
	2670809356656 [label="running_var
 (256)" fillcolor=orange]
	2670811690992 -> 2670809356464 [dir=none]
	2670809356464 [label="weight
 (256)" fillcolor=orange]
	2670811690992 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670811691136 -> 2670811690992
	2670811691136 -> 2670865671056 [dir=none]
	2670865671056 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2670811691136 -> 2670809356368 [dir=none]
	2670809356368 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2670811691136 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670811691328 -> 2670811691136
	2670811691328 -> 2670865713104 [dir=none]
	2670865713104 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2670811691328 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670811691472 -> 2670811691328
	2670811691472 -> 2670865667504 [dir=none]
	2670865667504 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2670811691472 -> 2670865713392 [dir=none]
	2670865713392 [label="result1
 (0)" fillcolor=orange]
	2670811691472 -> 2670865713776 [dir=none]
	2670865713776 [label="result2
 (0)" fillcolor=orange]
	2670811691472 -> 2670809355696 [dir=none]
	2670809355696 [label="running_mean
 (256)" fillcolor=orange]
	2670811691472 -> 2670809356080 [dir=none]
	2670809356080 [label="running_var
 (256)" fillcolor=orange]
	2670811691472 -> 2670809355888 [dir=none]
	2670809355888 [label="weight
 (256)" fillcolor=orange]
	2670811691472 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670811691568 -> 2670811691472
	2670811691568 -> 2670865670288 [dir=none]
	2670865670288 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2670811691568 -> 2670809355792 [dir=none]
	2670809355792 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2670811691568 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670811690944 -> 2670811691568
	2670811690944 -> 2670865714544 [dir=none]
	2670865714544 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2670811690944 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670811691856 -> 2670811690944
	2670811691856 [label="AddBackward0
------------
alpha: 1"]
	2670811691952 -> 2670811691856
	2670811691952 -> 2670865671344 [dir=none]
	2670865671344 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2670811691952 -> 2670865715024 [dir=none]
	2670865715024 [label="result1
 (0)" fillcolor=orange]
	2670811691952 -> 2670865715312 [dir=none]
	2670865715312 [label="result2
 (0)" fillcolor=orange]
	2670811691952 -> 2670809355120 [dir=none]
	2670809355120 [label="running_mean
 (256)" fillcolor=orange]
	2670811691952 -> 2670809355504 [dir=none]
	2670809355504 [label="running_var
 (256)" fillcolor=orange]
	2670811691952 -> 2670809355312 [dir=none]
	2670809355312 [label="weight
 (256)" fillcolor=orange]
	2670811691952 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670811692096 -> 2670811691952
	2670811692096 -> 2670865671824 [dir=none]
	2670865671824 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2670811692096 -> 2670809355216 [dir=none]
	2670809355216 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2670811692096 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670811692288 -> 2670811692096
	2670811692288 -> 2670865716080 [dir=none]
	2670865716080 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2670811692288 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670811692432 -> 2670811692288
	2670811692432 -> 2670865671920 [dir=none]
	2670865671920 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2670811692432 -> 2670865716368 [dir=none]
	2670865716368 [label="result1
 (0)" fillcolor=orange]
	2670811692432 -> 2670865716752 [dir=none]
	2670865716752 [label="result2
 (0)" fillcolor=orange]
	2670811692432 -> 2670809354544 [dir=none]
	2670809354544 [label="running_mean
 (256)" fillcolor=orange]
	2670811692432 -> 2670809354928 [dir=none]
	2670809354928 [label="running_var
 (256)" fillcolor=orange]
	2670811692432 -> 2670809354736 [dir=none]
	2670809354736 [label="weight
 (256)" fillcolor=orange]
	2670811692432 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670811692528 -> 2670811692432
	2670811692528 -> 2670865672400 [dir=none]
	2670865672400 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2670811692528 -> 2670809354640 [dir=none]
	2670809354640 [label="weight
 (256, 128, 3, 3)" fillcolor=orange]
	2670811692528 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2670811692720 -> 2670811692528
	2670811692720 -> 2670865717520 [dir=none]
	2670865717520 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2670811692720 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670811692864 -> 2670811692720
	2670811692864 [label="AddBackward0
------------
alpha: 1"]
	2670811692960 -> 2670811692864
	2670811692960 -> 2670865665008 [dir=none]
	2670865665008 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2670811692960 -> 2670865718000 [dir=none]
	2670865718000 [label="result1
 (0)" fillcolor=orange]
	2670811692960 -> 2670865718288 [dir=none]
	2670865718288 [label="result2
 (0)" fillcolor=orange]
	2670811692960 -> 2670809353392 [dir=none]
	2670809353392 [label="running_mean
 (128)" fillcolor=orange]
	2670811692960 -> 2670809353776 [dir=none]
	2670809353776 [label="running_var
 (128)" fillcolor=orange]
	2670811692960 -> 2670809353584 [dir=none]
	2670809353584 [label="weight
 (128)" fillcolor=orange]
	2670811692960 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670811693008 -> 2670811692960
	2670811693008 -> 2670865666448 [dir=none]
	2670865666448 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2670811693008 -> 2670809353488 [dir=none]
	2670809353488 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2670811693008 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670865727792 -> 2670811693008
	2670865727792 -> 2670865719056 [dir=none]
	2670865719056 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2670865727792 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670865727936 -> 2670865727792
	2670865727936 -> 2670865667312 [dir=none]
	2670865667312 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2670865727936 -> 2670865719344 [dir=none]
	2670865719344 [label="result1
 (0)" fillcolor=orange]
	2670865727936 -> 2670865719728 [dir=none]
	2670865719728 [label="result2
 (0)" fillcolor=orange]
	2670865727936 -> 2670809352816 [dir=none]
	2670809352816 [label="running_mean
 (128)" fillcolor=orange]
	2670865727936 -> 2670809353200 [dir=none]
	2670809353200 [label="running_var
 (128)" fillcolor=orange]
	2670865727936 -> 2670809353008 [dir=none]
	2670809353008 [label="weight
 (128)" fillcolor=orange]
	2670865727936 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670865728032 -> 2670865727936
	2670865728032 -> 2670865663664 [dir=none]
	2670865663664 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2670865728032 -> 2670809352912 [dir=none]
	2670809352912 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2670865728032 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670811692912 -> 2670865728032
	2670811692912 -> 2670865720496 [dir=none]
	2670865720496 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2670811692912 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670865728320 -> 2670811692912
	2670865728320 [label="AddBackward0
------------
alpha: 1"]
	2670865728416 -> 2670865728320
	2670865728416 -> 2670865662320 [dir=none]
	2670865662320 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2670865728416 -> 2670865720976 [dir=none]
	2670865720976 [label="result1
 (0)" fillcolor=orange]
	2670865728416 -> 2670865721264 [dir=none]
	2670865721264 [label="result2
 (0)" fillcolor=orange]
	2670865728416 -> 2670809352240 [dir=none]
	2670809352240 [label="running_mean
 (128)" fillcolor=orange]
	2670865728416 -> 2670809352624 [dir=none]
	2670809352624 [label="running_var
 (128)" fillcolor=orange]
	2670865728416 -> 2670809352432 [dir=none]
	2670809352432 [label="weight
 (128)" fillcolor=orange]
	2670865728416 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670865728560 -> 2670865728416
	2670865728560 -> 2670865664240 [dir=none]
	2670865664240 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2670865728560 -> 2670809352336 [dir=none]
	2670809352336 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2670865728560 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670865728752 -> 2670865728560
	2670865728752 -> 2670865722032 [dir=none]
	2670865722032 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2670865728752 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670865728896 -> 2670865728752
	2670865728896 -> 2670865672304 [dir=none]
	2670865672304 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2670865728896 -> 2670865722320 [dir=none]
	2670865722320 [label="result1
 (0)" fillcolor=orange]
	2670865728896 -> 2670865722704 [dir=none]
	2670865722704 [label="result2
 (0)" fillcolor=orange]
	2670865728896 -> 2670809351664 [dir=none]
	2670809351664 [label="running_mean
 (128)" fillcolor=orange]
	2670865728896 -> 2670809352048 [dir=none]
	2670809352048 [label="running_var
 (128)" fillcolor=orange]
	2670865728896 -> 2670809351856 [dir=none]
	2670809351856 [label="weight
 (128)" fillcolor=orange]
	2670865728896 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670865728992 -> 2670865728896
	2670865728992 -> 2670865664432 [dir=none]
	2670865664432 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2670865728992 -> 2670809351760 [dir=none]
	2670809351760 [label="weight
 (128, 64, 3, 3)" fillcolor=orange]
	2670865728992 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2670865729184 -> 2670865728992
	2670865729184 -> 2670865723472 [dir=none]
	2670865723472 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2670865729184 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670865729328 -> 2670865729184
	2670865729328 [label="AddBackward0
------------
alpha: 1"]
	2670865729424 -> 2670865729328
	2670865729424 -> 2670865672592 [dir=none]
	2670865672592 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2670865729424 -> 2670865723952 [dir=none]
	2670865723952 [label="result1
 (0)" fillcolor=orange]
	2670865729424 -> 2670865724240 [dir=none]
	2670865724240 [label="result2
 (0)" fillcolor=orange]
	2670865729424 -> 2670809350512 [dir=none]
	2670809350512 [label="running_mean
 (64)" fillcolor=orange]
	2670865729424 -> 2670809350896 [dir=none]
	2670809350896 [label="running_var
 (64)" fillcolor=orange]
	2670865729424 -> 2670809350704 [dir=none]
	2670809350704 [label="weight
 (64)" fillcolor=orange]
	2670865729424 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670865729568 -> 2670865729424
	2670865729568 -> 2670865670672 [dir=none]
	2670865670672 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2670865729568 -> 2670809350608 [dir=none]
	2670809350608 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2670865729568 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670865729760 -> 2670865729568
	2670865729760 -> 2670865725008 [dir=none]
	2670865725008 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2670865729760 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670865729904 -> 2670865729760
	2670865729904 -> 2670865671728 [dir=none]
	2670865671728 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2670865729904 -> 2670865725296 [dir=none]
	2670865725296 [label="result1
 (0)" fillcolor=orange]
	2670865729904 -> 2670865725680 [dir=none]
	2670865725680 [label="result2
 (0)" fillcolor=orange]
	2670865729904 -> 2670809284336 [dir=none]
	2670809284336 [label="running_mean
 (64)" fillcolor=orange]
	2670865729904 -> 2670809350320 [dir=none]
	2670809350320 [label="running_var
 (64)" fillcolor=orange]
	2670865729904 -> 2670809284528 [dir=none]
	2670809284528 [label="weight
 (64)" fillcolor=orange]
	2670865729904 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670865730000 -> 2670865729904
	2670865730000 -> 2670865381904 [dir=none]
	2670865381904 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2670865730000 -> 2670809284432 [dir=none]
	2670809284432 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2670865730000 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670865729376 -> 2670865730000
	2670865729376 -> 2670865726448 [dir=none]
	2670865726448 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2670865729376 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670865730288 -> 2670865729376
	2670865730288 [label="AddBackward0
------------
alpha: 1"]
	2670865730384 -> 2670865730288
	2670865730384 -> 2670865525232 [dir=none]
	2670865525232 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2670865730384 -> 2670865726928 [dir=none]
	2670865726928 [label="result1
 (0)" fillcolor=orange]
	2670865730384 -> 2670865727216 [dir=none]
	2670865727216 [label="result2
 (0)" fillcolor=orange]
	2670865730384 -> 2670809283760 [dir=none]
	2670809283760 [label="running_mean
 (64)" fillcolor=orange]
	2670865730384 -> 2670809284144 [dir=none]
	2670809284144 [label="running_var
 (64)" fillcolor=orange]
	2670865730384 -> 2670809283952 [dir=none]
	2670809283952 [label="weight
 (64)" fillcolor=orange]
	2670865730384 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670865730528 -> 2670865730384
	2670865730528 -> 2670865523984 [dir=none]
	2670865523984 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2670865730528 -> 2670809283856 [dir=none]
	2670809283856 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2670865730528 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670865730720 -> 2670865730528
	2670865730720 -> 2670865777200 [dir=none]
	2670865777200 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2670865730720 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670865730864 -> 2670865730720
	2670865730864 -> 2670865515824 [dir=none]
	2670865515824 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2670865730864 -> 2670865777488 [dir=none]
	2670865777488 [label="result1
 (0)" fillcolor=orange]
	2670865730864 -> 2670865777872 [dir=none]
	2670865777872 [label="result2
 (0)" fillcolor=orange]
	2670865730864 -> 2670809283184 [dir=none]
	2670809283184 [label="running_mean
 (64)" fillcolor=orange]
	2670865730864 -> 2670809283568 [dir=none]
	2670809283568 [label="running_var
 (64)" fillcolor=orange]
	2670865730864 -> 2670809283376 [dir=none]
	2670809283376 [label="weight
 (64)" fillcolor=orange]
	2670865730864 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670865730960 -> 2670865730864
	2670865730960 -> 2670865524080 [dir=none]
	2670865524080 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2670865730960 -> 2670809283280 [dir=none]
	2670809283280 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2670865730960 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2670865730336 -> 2670865730960
	2670865730336 -> 2670865778640 [dir=none]
	2670865778640 [label="result1
 (1, 64, 56, 56)" fillcolor=orange]
	2670865730336 -> 2670865526096 [dir=none]
	2670865526096 [label="self
 (1, 64, 112, 112)" fillcolor=orange]
	2670865730336 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:         (3, 3)
padding    :         (1, 1)
result1    : [saved tensor]
self       : [saved tensor]
stride     :         (2, 2)"]
	2670865731248 -> 2670865730336
	2670865731248 -> 2670865779024 [dir=none]
	2670865779024 [label="result
 (1, 64, 112, 112)" fillcolor=orange]
	2670865731248 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2670865731344 -> 2670865731248
	2670865731344 -> 2670865522640 [dir=none]
	2670865522640 [label="input
 (1, 64, 112, 112)" fillcolor=orange]
	2670865731344 -> 2670865779312 [dir=none]
	2670865779312 [label="result1
 (0)" fillcolor=orange]
	2670865731344 -> 2670865779696 [dir=none]
	2670865779696 [label="result2
 (0)" fillcolor=orange]
	2670865731344 -> 2670809279056 [dir=none]
	2670809279056 [label="running_mean
 (64)" fillcolor=orange]
	2670865731344 -> 2670809282992 [dir=none]
	2670809282992 [label="running_var
 (64)" fillcolor=orange]
	2670865731344 -> 2670809282800 [dir=none]
	2670809282800 [label="weight
 (64)" fillcolor=orange]
	2670865731344 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670865731440 -> 2670865731344
	2670865731440 -> 2670865523696 [dir=none]
	2670865523696 [label="input
 (1, 3, 224, 224)" fillcolor=orange]
	2670865731440 -> 2670809282704 [dir=none]
	2670809282704 [label="weight
 (64, 3, 7, 7)" fillcolor=orange]
	2670865731440 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2670865731632 -> 2670865731440
	2670809282704 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2670809282704 -> 2670865731632
	2670865731632 [label=AccumulateGrad]
	2670865731392 -> 2670865731344
	2670809282800 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2670809282800 -> 2670865731392
	2670865731392 [label=AccumulateGrad]
	2670865731056 -> 2670865731344
	2670809282896 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2670809282896 -> 2670865731056
	2670865731056 [label=AccumulateGrad]
	2670865731152 -> 2670865730960
	2670809283280 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2670809283280 -> 2670865731152
	2670865731152 [label=AccumulateGrad]
	2670865730912 -> 2670865730864
	2670809283376 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2670809283376 -> 2670865730912
	2670865730912 [label=AccumulateGrad]
	2670865730768 -> 2670865730864
	2670809283472 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2670809283472 -> 2670865730768
	2670865730768 [label=AccumulateGrad]
	2670865730672 -> 2670865730528
	2670809283856 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2670809283856 -> 2670865730672
	2670865730672 [label=AccumulateGrad]
	2670865730480 -> 2670865730384
	2670809283952 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2670809283952 -> 2670865730480
	2670865730480 [label=AccumulateGrad]
	2670865730432 -> 2670865730384
	2670809284048 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2670809284048 -> 2670865730432
	2670865730432 [label=AccumulateGrad]
	2670865730336 -> 2670865730288
	2670865730192 -> 2670865730000
	2670809284432 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2670809284432 -> 2670865730192
	2670865730192 [label=AccumulateGrad]
	2670865729952 -> 2670865729904
	2670809284528 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2670809284528 -> 2670865729952
	2670865729952 [label=AccumulateGrad]
	2670865729808 -> 2670865729904
	2670809350224 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2670809350224 -> 2670865729808
	2670865729808 [label=AccumulateGrad]
	2670865729712 -> 2670865729568
	2670809350608 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2670809350608 -> 2670865729712
	2670865729712 [label=AccumulateGrad]
	2670865729520 -> 2670865729424
	2670809350704 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2670809350704 -> 2670865729520
	2670865729520 [label=AccumulateGrad]
	2670865729472 -> 2670865729424
	2670809350800 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2670809350800 -> 2670865729472
	2670865729472 [label=AccumulateGrad]
	2670865729376 -> 2670865729328
	2670865729136 -> 2670865728992
	2670809351760 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2670809351760 -> 2670865729136
	2670865729136 [label=AccumulateGrad]
	2670865728944 -> 2670865728896
	2670809351856 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2670809351856 -> 2670865728944
	2670865728944 [label=AccumulateGrad]
	2670865728800 -> 2670865728896
	2670809351952 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2670809351952 -> 2670865728800
	2670865728800 [label=AccumulateGrad]
	2670865728704 -> 2670865728560
	2670809352336 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2670809352336 -> 2670865728704
	2670865728704 [label=AccumulateGrad]
	2670865728512 -> 2670865728416
	2670809352432 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2670809352432 -> 2670865728512
	2670865728512 [label=AccumulateGrad]
	2670865728464 -> 2670865728416
	2670809352528 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2670809352528 -> 2670865728464
	2670865728464 [label=AccumulateGrad]
	2670865728368 -> 2670865728320
	2670865728368 -> 2670865666736 [dir=none]
	2670865666736 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2670865728368 -> 2670865786800 [dir=none]
	2670865786800 [label="result1
 (0)" fillcolor=orange]
	2670865728368 -> 2670865787088 [dir=none]
	2670865787088 [label="result2
 (0)" fillcolor=orange]
	2670865728368 -> 2670809351088 [dir=none]
	2670809351088 [label="running_mean
 (128)" fillcolor=orange]
	2670865728368 -> 2670809351472 [dir=none]
	2670809351472 [label="running_var
 (128)" fillcolor=orange]
	2670865728368 -> 2670809351280 [dir=none]
	2670809351280 [label="weight
 (128)" fillcolor=orange]
	2670865728368 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670865729088 -> 2670865728368
	2670865729088 -> 2670865664432 [dir=none]
	2670865664432 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2670865729088 -> 2670809351184 [dir=none]
	2670809351184 [label="weight
 (128, 64, 1, 1)" fillcolor=orange]
	2670865729088 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2670865729184 -> 2670865729088
	2670865729232 -> 2670865729088
	2670809351184 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2670809351184 -> 2670865729232
	2670865729232 [label=AccumulateGrad]
	2670865728656 -> 2670865728368
	2670809351280 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2670809351280 -> 2670865728656
	2670865728656 [label=AccumulateGrad]
	2670865728608 -> 2670865728368
	2670809351376 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2670809351376 -> 2670865728608
	2670865728608 [label=AccumulateGrad]
	2670865728224 -> 2670865728032
	2670809352912 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2670809352912 -> 2670865728224
	2670865728224 [label=AccumulateGrad]
	2670865727984 -> 2670865727936
	2670809353008 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2670809353008 -> 2670865727984
	2670865727984 [label=AccumulateGrad]
	2670865727840 -> 2670865727936
	2670809353104 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2670809353104 -> 2670865727840
	2670865727840 [label=AccumulateGrad]
	2670865727744 -> 2670811693008
	2670809353488 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2670809353488 -> 2670865727744
	2670865727744 [label=AccumulateGrad]
	2670865727600 -> 2670811692960
	2670809353584 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2670809353584 -> 2670865727600
	2670865727600 [label=AccumulateGrad]
	2670865727552 -> 2670811692960
	2670809353680 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2670809353680 -> 2670865727552
	2670865727552 [label=AccumulateGrad]
	2670811692912 -> 2670811692864
	2670811692672 -> 2670811692528
	2670809354640 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2670809354640 -> 2670811692672
	2670811692672 [label=AccumulateGrad]
	2670811692480 -> 2670811692432
	2670809354736 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2670809354736 -> 2670811692480
	2670811692480 [label=AccumulateGrad]
	2670811692336 -> 2670811692432
	2670809354832 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2670809354832 -> 2670811692336
	2670811692336 [label=AccumulateGrad]
	2670811692240 -> 2670811692096
	2670809355216 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2670809355216 -> 2670811692240
	2670811692240 [label=AccumulateGrad]
	2670811692048 -> 2670811691952
	2670809355312 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2670809355312 -> 2670811692048
	2670811692048 [label=AccumulateGrad]
	2670811692000 -> 2670811691952
	2670809355408 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2670809355408 -> 2670811692000
	2670811692000 [label=AccumulateGrad]
	2670811691904 -> 2670811691856
	2670811691904 -> 2670865668560 [dir=none]
	2670865668560 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2670811691904 -> 2670865792464 [dir=none]
	2670865792464 [label="result1
 (0)" fillcolor=orange]
	2670811691904 -> 2670865792752 [dir=none]
	2670865792752 [label="result2
 (0)" fillcolor=orange]
	2670811691904 -> 2670809353968 [dir=none]
	2670809353968 [label="running_mean
 (256)" fillcolor=orange]
	2670811691904 -> 2670809354352 [dir=none]
	2670809354352 [label="running_var
 (256)" fillcolor=orange]
	2670811691904 -> 2670809354160 [dir=none]
	2670809354160 [label="weight
 (256)" fillcolor=orange]
	2670811691904 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670811692624 -> 2670811691904
	2670811692624 -> 2670865672400 [dir=none]
	2670865672400 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2670811692624 -> 2670809354064 [dir=none]
	2670809354064 [label="weight
 (256, 128, 1, 1)" fillcolor=orange]
	2670811692624 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2670811692720 -> 2670811692624
	2670811692816 -> 2670811692624
	2670809354064 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2670809354064 -> 2670811692816
	2670811692816 [label=AccumulateGrad]
	2670811692192 -> 2670811691904
	2670809354160 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2670809354160 -> 2670811692192
	2670811692192 [label=AccumulateGrad]
	2670811692144 -> 2670811691904
	2670809354256 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2670809354256 -> 2670811692144
	2670811692144 [label=AccumulateGrad]
	2670811691760 -> 2670811691568
	2670809355792 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2670809355792 -> 2670811691760
	2670811691760 [label=AccumulateGrad]
	2670811691520 -> 2670811691472
	2670809355888 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2670809355888 -> 2670811691520
	2670811691520 [label=AccumulateGrad]
	2670811691376 -> 2670811691472
	2670809355984 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2670809355984 -> 2670811691376
	2670811691376 [label=AccumulateGrad]
	2670811691280 -> 2670811691136
	2670809356368 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2670809356368 -> 2670811691280
	2670811691280 [label=AccumulateGrad]
	2670811691088 -> 2670811690992
	2670809356464 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2670809356464 -> 2670811691088
	2670811691088 [label=AccumulateGrad]
	2670811691040 -> 2670811690992
	2670809356560 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2670809356560 -> 2670811691040
	2670811691040 [label=AccumulateGrad]
	2670811690944 -> 2670811690896
	2670811690704 -> 2670811690560
	2670809357520 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2670809357520 -> 2670811690704
	2670811690704 [label=AccumulateGrad]
	2670811690512 -> 2670811690464
	2670809357616 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2670809357616 -> 2670811690512
	2670811690512 [label=AccumulateGrad]
	2670811690368 -> 2670811690464
	2670809357712 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2670809357712 -> 2670811690368
	2670811690368 [label=AccumulateGrad]
	2670811690272 -> 2670811690128
	2670809358096 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2670809358096 -> 2670811690272
	2670811690272 [label=AccumulateGrad]
	2670811690080 -> 2670811689984
	2670809358192 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2670809358192 -> 2670811690080
	2670811690080 [label=AccumulateGrad]
	2670811690032 -> 2670811689984
	2670809358288 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2670809358288 -> 2670811690032
	2670811690032 [label=AccumulateGrad]
	2670811689936 -> 2670811689888
	2670811689936 -> 2670865670096 [dir=none]
	2670865670096 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2670811689936 -> 2670865798192 [dir=none]
	2670865798192 [label="result1
 (0)" fillcolor=orange]
	2670811689936 -> 2670865798480 [dir=none]
	2670865798480 [label="result2
 (0)" fillcolor=orange]
	2670811689936 -> 2670809356848 [dir=none]
	2670809356848 [label="running_mean
 (512)" fillcolor=orange]
	2670811689936 -> 2670809357232 [dir=none]
	2670809357232 [label="running_var
 (512)" fillcolor=orange]
	2670811689936 -> 2670809357040 [dir=none]
	2670809357040 [label="weight
 (512)" fillcolor=orange]
	2670811689936 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2670811690656 -> 2670811689936
	2670811690656 -> 2670865669808 [dir=none]
	2670865669808 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2670811690656 -> 2670809356944 [dir=none]
	2670809356944 [label="weight
 (512, 256, 1, 1)" fillcolor=orange]
	2670811690656 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2670811690752 -> 2670811690656
	2670811690800 -> 2670811690656
	2670809356944 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2670809356944 -> 2670811690800
	2670811690800 [label=AccumulateGrad]
	2670811690224 -> 2670811689936
	2670809357040 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2670809357040 -> 2670811690224
	2670811690224 [label=AccumulateGrad]
	2670811690176 -> 2670811689936
	2670809357136 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2670809357136 -> 2670811690176
	2670811690176 [label=AccumulateGrad]
	2670811689792 -> 2670811689600
	2670809358672 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2670809358672 -> 2670811689792
	2670811689792 [label=AccumulateGrad]
	2670811689552 -> 2670811689504
	2670809358768 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2670809358768 -> 2670811689552
	2670811689552 [label=AccumulateGrad]
	2670811689408 -> 2670811689504
	2670809358864 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2670809358864 -> 2670811689408
	2670811689408 [label=AccumulateGrad]
	2670811689312 -> 2670811689168
	2670809359248 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2670809359248 -> 2670811689312
	2670811689312 [label=AccumulateGrad]
	2670811689120 -> 2670811688784
	2670809359344 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2670809359344 -> 2670811689120
	2670811689120 [label=AccumulateGrad]
	2670811689072 -> 2670811688784
	2670809359440 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2670809359440 -> 2670811689072
	2670811689072 [label=AccumulateGrad]
	2670811688832 -> 2670811688880
	2670811688640 -> 2670811687776
	2670811688640 [label=TBackward0]
	2670811688928 -> 2670811688640
	2670809366256 [label="fc.weight
 (2, 512)" fillcolor=lightblue]
	2670809366256 -> 2670811688928
	2670811688928 [label=AccumulateGrad]
	2670811687776 -> 2670865671248
}
