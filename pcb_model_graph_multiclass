digraph {
	graph [size="113.85,113.85"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2674022348864 [label="
 (1, 6)" fillcolor=darkolivegreen1]
	2673980053760 -> 2674022348784 [dir=none]
	2674022348784 [label="mat1
 (1, 512)" fillcolor=orange]
	2673980053760 -> 2674022348944 [dir=none]
	2674022348944 [label="mat2
 (512, 6)" fillcolor=orange]
	2673980053760 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 512)
mat1_sym_strides:       (512, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (512, 6)
mat2_sym_strides:       (1, 512)"]
	2673980058032 -> 2673980053760
	2673875500752 [label="fc.bias
 (6)" fillcolor=lightblue]
	2673875500752 -> 2673980058032
	2673980058032 [label=AccumulateGrad]
	2673980049200 -> 2673980053760
	2673980049200 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 512, 1, 1)"]
	2673980054864 -> 2673980049200
	2673980054864 [label="MeanBackward1
----------------------------------------
dim           : (4294967295, 4294967294)
keepdim       :                     True
self_sym_numel:                    25088
self_sym_sizes:           (1, 512, 7, 7)"]
	2673980057936 -> 2673980054864
	2673980057936 -> 2674022350224 [dir=none]
	2674022350224 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2673980057936 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2673980050688 -> 2673980057936
	2673980050688 [label="AddBackward0
------------
alpha: 1"]
	2673875080944 -> 2673980050688
	2673875080944 -> 2674022348704 [dir=none]
	2674022348704 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2673875080944 -> 2674022350384 [dir=none]
	2674022350384 [label="result1
 (0)" fillcolor=orange]
	2673875080944 -> 2674022350544 [dir=none]
	2674022350544 [label="result2
 (0)" fillcolor=orange]
	2673875080944 -> 2673875355680 [dir=none]
	2673875355680 [label="running_mean
 (512)" fillcolor=orange]
	2673875080944 -> 2673875356080 [dir=none]
	2673875356080 [label="running_var
 (512)" fillcolor=orange]
	2673875080944 -> 2673875355840 [dir=none]
	2673875355840 [label="weight
 (512)" fillcolor=orange]
	2673875080944 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673871198704 -> 2673875080944
	2673871198704 -> 2674022348464 [dir=none]
	2674022348464 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2673871198704 -> 2673875355920 [dir=none]
	2673875355920 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2673871198704 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2673875089344 -> 2673871198704
	2673875089344 -> 2674022350784 [dir=none]
	2674022350784 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2673875089344 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2673980049344 -> 2673875089344
	2673980049344 -> 2674022348624 [dir=none]
	2674022348624 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2673980049344 -> 2674022416624 [dir=none]
	2674022416624 [label="result1
 (0)" fillcolor=orange]
	2673980049344 -> 2674022416464 [dir=none]
	2674022416464 [label="result2
 (0)" fillcolor=orange]
	2673980049344 -> 2673875355040 [dir=none]
	2673875355040 [label="running_mean
 (512)" fillcolor=orange]
	2673980049344 -> 2673875355440 [dir=none]
	2673875355440 [label="running_var
 (512)" fillcolor=orange]
	2673980049344 -> 2673875355200 [dir=none]
	2673875355200 [label="weight
 (512)" fillcolor=orange]
	2673980049344 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673980054528 -> 2673980049344
	2673980054528 -> 2674022348384 [dir=none]
	2674022348384 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2673980054528 -> 2673875355280 [dir=none]
	2673875355280 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2673980054528 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2673875083008 -> 2673980054528
	2673875083008 -> 2674022416944 [dir=none]
	2674022416944 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2673875083008 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2673927059248 -> 2673875083008
	2673927059248 [label="AddBackward0
------------
alpha: 1"]
	2673927050320 -> 2673927059248
	2673927050320 -> 2674022348304 [dir=none]
	2674022348304 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2673927050320 -> 2674022417104 [dir=none]
	2674022417104 [label="result1
 (0)" fillcolor=orange]
	2673927050320 -> 2674022417344 [dir=none]
	2674022417344 [label="result2
 (0)" fillcolor=orange]
	2673927050320 -> 2673875354400 [dir=none]
	2673875354400 [label="running_mean
 (512)" fillcolor=orange]
	2673927050320 -> 2673875354800 [dir=none]
	2673875354800 [label="running_var
 (512)" fillcolor=orange]
	2673927050320 -> 2673875354560 [dir=none]
	2673875354560 [label="weight
 (512)" fillcolor=orange]
	2673927050320 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673927058144 -> 2673927050320
	2673927058144 -> 2674022348224 [dir=none]
	2674022348224 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2673927058144 -> 2673875354640 [dir=none]
	2673875354640 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2673927058144 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2673980162544 -> 2673927058144
	2673980162544 -> 2674022417584 [dir=none]
	2674022417584 [label="result
 (1, 512, 7, 7)" fillcolor=orange]
	2673980162544 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2673980168496 -> 2673980162544
	2673980168496 -> 2674022348144 [dir=none]
	2674022348144 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2673980168496 -> 2674022417824 [dir=none]
	2674022417824 [label="result1
 (0)" fillcolor=orange]
	2673980168496 -> 2674022417664 [dir=none]
	2674022417664 [label="result2
 (0)" fillcolor=orange]
	2673980168496 -> 2673875353760 [dir=none]
	2673875353760 [label="running_mean
 (512)" fillcolor=orange]
	2673980168496 -> 2673875354160 [dir=none]
	2673875354160 [label="running_var
 (512)" fillcolor=orange]
	2673980168496 -> 2673875353920 [dir=none]
	2673875353920 [label="weight
 (512)" fillcolor=orange]
	2673980168496 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673980163312 -> 2673980168496
	2673980163312 -> 2674022348064 [dir=none]
	2674022348064 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2673980163312 -> 2673875354000 [dir=none]
	2673875354000 [label="weight
 (512, 256, 3, 3)" fillcolor=orange]
	2673980163312 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2673980173920 -> 2673980163312
	2673980173920 -> 2674022417904 [dir=none]
	2674022417904 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2673980173920 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2673980169264 -> 2673980173920
	2673980169264 [label="AddBackward0
------------
alpha: 1"]
	2673980170272 -> 2673980169264
	2673980170272 -> 2674022347984 [dir=none]
	2674022347984 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2673980170272 -> 2674022418384 [dir=none]
	2674022418384 [label="result1
 (0)" fillcolor=orange]
	2673980170272 -> 2674022418304 [dir=none]
	2674022418304 [label="result2
 (0)" fillcolor=orange]
	2673980170272 -> 2673875221344 [dir=none]
	2673875221344 [label="running_mean
 (256)" fillcolor=orange]
	2673980170272 -> 2673875221744 [dir=none]
	2673875221744 [label="running_var
 (256)" fillcolor=orange]
	2673980170272 -> 2673875221504 [dir=none]
	2673875221504 [label="weight
 (256)" fillcolor=orange]
	2673980170272 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673980169072 -> 2673980170272
	2673980169072 -> 2674022347744 [dir=none]
	2674022347744 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2673980169072 -> 2673875221584 [dir=none]
	2673875221584 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2673980169072 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2673980165904 -> 2673980169072
	2673980165904 -> 2674022418464 [dir=none]
	2674022418464 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2673980165904 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2673980163024 -> 2673980165904
	2673980163024 -> 2674022347904 [dir=none]
	2674022347904 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2673980163024 -> 2674022418864 [dir=none]
	2674022418864 [label="result1
 (0)" fillcolor=orange]
	2673980163024 -> 2674022418784 [dir=none]
	2674022418784 [label="result2
 (0)" fillcolor=orange]
	2673980163024 -> 2673875220704 [dir=none]
	2673875220704 [label="running_mean
 (256)" fillcolor=orange]
	2673980163024 -> 2673875221104 [dir=none]
	2673875221104 [label="running_var
 (256)" fillcolor=orange]
	2673980163024 -> 2673875220864 [dir=none]
	2673875220864 [label="weight
 (256)" fillcolor=orange]
	2673980163024 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673980162592 -> 2673980163024
	2673980162592 -> 2674022347664 [dir=none]
	2674022347664 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2673980162592 -> 2673875220944 [dir=none]
	2673875220944 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2673980162592 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2673980165520 -> 2673980162592
	2673980165520 -> 2674022418944 [dir=none]
	2674022418944 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2673980165520 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2673980170752 -> 2673980165520
	2673980170752 [label="AddBackward0
------------
alpha: 1"]
	2673980172288 -> 2673980170752
	2673980172288 -> 2674022347584 [dir=none]
	2674022347584 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2673980172288 -> 2674022419424 [dir=none]
	2674022419424 [label="result1
 (0)" fillcolor=orange]
	2673980172288 -> 2674022419344 [dir=none]
	2674022419344 [label="result2
 (0)" fillcolor=orange]
	2673980172288 -> 2673875220064 [dir=none]
	2673875220064 [label="running_mean
 (256)" fillcolor=orange]
	2673980172288 -> 2673875220464 [dir=none]
	2673875220464 [label="running_var
 (256)" fillcolor=orange]
	2673980172288 -> 2673875220224 [dir=none]
	2673875220224 [label="weight
 (256)" fillcolor=orange]
	2673980172288 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673980171472 -> 2673980172288
	2673980171472 -> 2674022347504 [dir=none]
	2674022347504 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2673980171472 -> 2673875220304 [dir=none]
	2673875220304 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2673980171472 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2673980172768 -> 2673980171472
	2673980172768 -> 2674022419504 [dir=none]
	2674022419504 [label="result
 (1, 256, 14, 14)" fillcolor=orange]
	2673980172768 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2673980177712 -> 2673980172768
	2673980177712 -> 2674022347424 [dir=none]
	2674022347424 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2673980177712 -> 2674022419904 [dir=none]
	2674022419904 [label="result1
 (0)" fillcolor=orange]
	2673980177712 -> 2674022419824 [dir=none]
	2674022419824 [label="result2
 (0)" fillcolor=orange]
	2673980177712 -> 2673875219424 [dir=none]
	2673875219424 [label="running_mean
 (256)" fillcolor=orange]
	2673980177712 -> 2673875219824 [dir=none]
	2673875219824 [label="running_var
 (256)" fillcolor=orange]
	2673980177712 -> 2673875219584 [dir=none]
	2673875219584 [label="weight
 (256)" fillcolor=orange]
	2673980177712 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673980174304 -> 2673980177712
	2673980174304 -> 2674022347344 [dir=none]
	2674022347344 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2673980174304 -> 2673875219664 [dir=none]
	2673875219664 [label="weight
 (256, 128, 3, 3)" fillcolor=orange]
	2673980174304 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2673980172624 -> 2673980174304
	2673980172624 -> 2674022419984 [dir=none]
	2674022419984 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2673980172624 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2673980178144 -> 2673980172624
	2673980178144 [label="AddBackward0
------------
alpha: 1"]
	2673980177952 -> 2673980178144
	2673980177952 -> 2674022347264 [dir=none]
	2674022347264 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2673980177952 -> 2674022420464 [dir=none]
	2674022420464 [label="result1
 (0)" fillcolor=orange]
	2673980177952 -> 2674022420384 [dir=none]
	2674022420384 [label="result2
 (0)" fillcolor=orange]
	2673980177952 -> 2673875218144 [dir=none]
	2673875218144 [label="running_mean
 (128)" fillcolor=orange]
	2673980177952 -> 2673875218544 [dir=none]
	2673875218544 [label="running_var
 (128)" fillcolor=orange]
	2673980177952 -> 2673875218304 [dir=none]
	2673875218304 [label="weight
 (128)" fillcolor=orange]
	2673980177952 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673980177904 -> 2673980177952
	2673980177904 -> 2674022347024 [dir=none]
	2674022347024 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2673980177904 -> 2673875218384 [dir=none]
	2673875218384 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2673980177904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2673980177664 -> 2673980177904
	2673980177664 -> 2674022420544 [dir=none]
	2674022420544 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2673980177664 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2673980178240 -> 2673980177664
	2673980178240 -> 2674022347184 [dir=none]
	2674022347184 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2673980178240 -> 2674022420784 [dir=none]
	2674022420784 [label="result1
 (0)" fillcolor=orange]
	2673980178240 -> 2674022420944 [dir=none]
	2674022420944 [label="result2
 (0)" fillcolor=orange]
	2673980178240 -> 2673875217504 [dir=none]
	2673875217504 [label="running_mean
 (128)" fillcolor=orange]
	2673980178240 -> 2673875217904 [dir=none]
	2673875217904 [label="running_var
 (128)" fillcolor=orange]
	2673980178240 -> 2673875217664 [dir=none]
	2673875217664 [label="weight
 (128)" fillcolor=orange]
	2673980178240 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673980178048 -> 2673980178240
	2673980178048 -> 2674022346944 [dir=none]
	2674022346944 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2673980178048 -> 2673875217744 [dir=none]
	2673875217744 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2673980178048 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2673980178096 -> 2673980178048
	2673980178096 -> 2674022420864 [dir=none]
	2674022420864 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2673980178096 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2674022466368 -> 2673980178096
	2674022466368 [label="AddBackward0
------------
alpha: 1"]
	2674022466176 -> 2674022466368
	2674022466176 -> 2674022346864 [dir=none]
	2674022346864 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2674022466176 -> 2674022421424 [dir=none]
	2674022421424 [label="result1
 (0)" fillcolor=orange]
	2674022466176 -> 2674022421344 [dir=none]
	2674022421344 [label="result2
 (0)" fillcolor=orange]
	2674022466176 -> 2673875216864 [dir=none]
	2673875216864 [label="running_mean
 (128)" fillcolor=orange]
	2674022466176 -> 2673875217264 [dir=none]
	2673875217264 [label="running_var
 (128)" fillcolor=orange]
	2674022466176 -> 2673875217024 [dir=none]
	2673875217024 [label="weight
 (128)" fillcolor=orange]
	2674022466176 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2674022466128 -> 2674022466176
	2674022466128 -> 2674022346784 [dir=none]
	2674022346784 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2674022466128 -> 2673875217104 [dir=none]
	2673875217104 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2674022466128 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2674022466416 -> 2674022466128
	2674022466416 -> 2674022421504 [dir=none]
	2674022421504 [label="result
 (1, 128, 28, 28)" fillcolor=orange]
	2674022466416 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2674022466848 -> 2674022466416
	2674022466848 -> 2674022346704 [dir=none]
	2674022346704 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2674022466848 -> 2674022421904 [dir=none]
	2674022421904 [label="result1
 (0)" fillcolor=orange]
	2674022466848 -> 2674022421824 [dir=none]
	2674022421824 [label="result2
 (0)" fillcolor=orange]
	2674022466848 -> 2673875216144 [dir=none]
	2673875216144 [label="running_mean
 (128)" fillcolor=orange]
	2674022466848 -> 2673875216624 [dir=none]
	2673875216624 [label="running_var
 (128)" fillcolor=orange]
	2674022466848 -> 2673875216304 [dir=none]
	2673875216304 [label="weight
 (128)" fillcolor=orange]
	2674022466848 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2674022466656 -> 2674022466848
	2674022466656 -> 2674022346624 [dir=none]
	2674022346624 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2674022466656 -> 2673875216544 [dir=none]
	2673875216544 [label="weight
 (128, 64, 3, 3)" fillcolor=orange]
	2674022466656 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2674022465792 -> 2674022466656
	2674022465792 -> 2674022422224 [dir=none]
	2674022422224 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2674022465792 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2674022467280 -> 2674022465792
	2674022467280 [label="AddBackward0
------------
alpha: 1"]
	2674022467088 -> 2674022467280
	2674022467088 -> 2674022346544 [dir=none]
	2674022346544 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2674022467088 -> 2674022422464 [dir=none]
	2674022422464 [label="result1
 (0)" fillcolor=orange]
	2674022467088 -> 2674022420144 [dir=none]
	2674022420144 [label="result2
 (0)" fillcolor=orange]
	2674022467088 -> 2673875215024 [dir=none]
	2673875215024 [label="running_mean
 (64)" fillcolor=orange]
	2674022467088 -> 2673875215424 [dir=none]
	2673875215424 [label="running_var
 (64)" fillcolor=orange]
	2674022467088 -> 2673875215184 [dir=none]
	2673875215184 [label="weight
 (64)" fillcolor=orange]
	2674022467088 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2674022467040 -> 2674022467088
	2674022467040 -> 2674022346464 [dir=none]
	2674022346464 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2674022467040 -> 2673875215264 [dir=none]
	2673875215264 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2674022467040 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2674022466800 -> 2674022467040
	2674022466800 -> 2674022421104 [dir=none]
	2674022421104 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2674022466800 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2674022467856 -> 2674022466800
	2674022467856 -> 2674022346384 [dir=none]
	2674022346384 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2674022467856 -> 2674022422784 [dir=none]
	2674022422784 [label="result1
 (0)" fillcolor=orange]
	2674022467856 -> 2674022422944 [dir=none]
	2674022422944 [label="result2
 (0)" fillcolor=orange]
	2674022467856 -> 2673875213104 [dir=none]
	2673875213104 [label="running_mean
 (64)" fillcolor=orange]
	2674022467856 -> 2673875214784 [dir=none]
	2673875214784 [label="running_var
 (64)" fillcolor=orange]
	2674022467856 -> 2673875214544 [dir=none]
	2673875214544 [label="weight
 (64)" fillcolor=orange]
	2674022467856 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2674022467664 -> 2674022467856
	2674022467664 -> 2674022345824 [dir=none]
	2674022345824 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2674022467664 -> 2673875214624 [dir=none]
	2673875214624 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2674022467664 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2674022467232 -> 2674022467664
	2674022467232 -> 2674022423264 [dir=none]
	2674022423264 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2674022467232 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2674022468336 -> 2674022467232
	2674022468336 [label="AddBackward0
------------
alpha: 1"]
	2674022468144 -> 2674022468336
	2674022468144 -> 2674022346304 [dir=none]
	2674022346304 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2674022468144 -> 2674022423504 [dir=none]
	2674022423504 [label="result1
 (0)" fillcolor=orange]
	2674022468144 -> 2674022422704 [dir=none]
	2674022422704 [label="result2
 (0)" fillcolor=orange]
	2674022468144 -> 2673875213824 [dir=none]
	2673875213824 [label="running_mean
 (64)" fillcolor=orange]
	2674022468144 -> 2673875214224 [dir=none]
	2673875214224 [label="running_var
 (64)" fillcolor=orange]
	2674022468144 -> 2673875213984 [dir=none]
	2673875213984 [label="weight
 (64)" fillcolor=orange]
	2674022468144 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2674022468096 -> 2674022468144
	2674022468096 -> 2674022346144 [dir=none]
	2674022346144 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2674022468096 -> 2673875214064 [dir=none]
	2673875214064 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2674022468096 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2674022468384 -> 2674022468096
	2674022468384 -> 2674022422144 [dir=none]
	2674022422144 [label="result
 (1, 64, 56, 56)" fillcolor=orange]
	2674022468384 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2674022468816 -> 2674022468384
	2674022468816 -> 2674022346224 [dir=none]
	2674022346224 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2674022468816 -> 2674022423824 [dir=none]
	2674022423824 [label="result1
 (0)" fillcolor=orange]
	2674022468816 -> 2674022423984 [dir=none]
	2674022423984 [label="result2
 (0)" fillcolor=orange]
	2674022468816 -> 2673875213184 [dir=none]
	2673875213184 [label="running_mean
 (64)" fillcolor=orange]
	2674022468816 -> 2673875213584 [dir=none]
	2673875213584 [label="running_var
 (64)" fillcolor=orange]
	2674022468816 -> 2673875213344 [dir=none]
	2673875213344 [label="weight
 (64)" fillcolor=orange]
	2674022468816 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2674022468624 -> 2674022468816
	2674022468624 -> 2674022345504 [dir=none]
	2674022345504 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2674022468624 -> 2673875213424 [dir=none]
	2673875213424 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2674022468624 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2674022468288 -> 2674022468624
	2674022468288 -> 2674022345744 [dir=none]
	2674022345744 [label="result1
 (1, 64, 56, 56)" fillcolor=orange]
	2674022468288 -> 2674022345424 [dir=none]
	2674022345424 [label="self
 (1, 64, 112, 112)" fillcolor=orange]
	2674022468288 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:         (3, 3)
padding    :         (1, 1)
result1    : [saved tensor]
self       : [saved tensor]
stride     :         (2, 2)"]
	2674022469296 -> 2674022468288
	2674022469296 -> 2674022424384 [dir=none]
	2674022424384 [label="result
 (1, 64, 112, 112)" fillcolor=orange]
	2674022469296 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2674022468528 -> 2674022469296
	2674022468528 -> 2673980086608 [dir=none]
	2673980086608 [label="input
 (1, 64, 112, 112)" fillcolor=orange]
	2674022468528 -> 2674022424544 [dir=none]
	2674022424544 [label="result1
 (0)" fillcolor=orange]
	2674022468528 -> 2674022424704 [dir=none]
	2674022424704 [label="result2
 (0)" fillcolor=orange]
	2674022468528 -> 2673875212544 [dir=none]
	2673875212544 [label="running_mean
 (64)" fillcolor=orange]
	2674022468528 -> 2673875212864 [dir=none]
	2673875212864 [label="running_var
 (64)" fillcolor=orange]
	2674022468528 -> 2673875212704 [dir=none]
	2673875212704 [label="weight
 (64)" fillcolor=orange]
	2674022468528 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2674022469008 -> 2674022468528
	2674022469008 -> 2673980010208 [dir=none]
	2673980010208 [label="input
 (1, 3, 224, 224)" fillcolor=orange]
	2674022469008 -> 2673875212624 [dir=none]
	2673875212624 [label="weight
 (64, 3, 7, 7)" fillcolor=orange]
	2674022469008 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2674022469344 -> 2674022469008
	2673875212624 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2673875212624 -> 2674022469344
	2674022469344 [label=AccumulateGrad]
	2674022469056 -> 2674022468528
	2673875212704 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2673875212704 -> 2674022469056
	2674022469056 [label=AccumulateGrad]
	2674022467184 -> 2674022468528
	2673875212784 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2673875212784 -> 2674022467184
	2674022467184 [label=AccumulateGrad]
	2674022467568 -> 2674022468624
	2673875213424 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2673875213424 -> 2674022467568
	2674022467568 [label=AccumulateGrad]
	2674022468576 -> 2674022468816
	2673875213344 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2673875213344 -> 2674022468576
	2674022468576 [label=AccumulateGrad]
	2674022468864 -> 2674022468816
	2673875213504 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2673875213504 -> 2674022468864
	2674022468864 [label=AccumulateGrad]
	2674022468480 -> 2674022468096
	2673875214064 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2673875214064 -> 2674022468480
	2674022468480 [label=AccumulateGrad]
	2674022467760 -> 2674022468144
	2673875213984 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2673875213984 -> 2674022467760
	2674022467760 [label=AccumulateGrad]
	2674022468048 -> 2674022468144
	2673875214144 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2673875214144 -> 2674022468048
	2674022468048 [label=AccumulateGrad]
	2674022468288 -> 2674022468336
	2674022467376 -> 2674022467664
	2673875214624 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2673875214624 -> 2674022467376
	2674022467376 [label=AccumulateGrad]
	2674022467616 -> 2674022467856
	2673875214544 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2673875214544 -> 2674022467616
	2674022467616 [label=AccumulateGrad]
	2674022467904 -> 2674022467856
	2673875214704 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2673875214704 -> 2674022467904
	2674022467904 [label=AccumulateGrad]
	2674022467520 -> 2674022467040
	2673875215264 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2673875215264 -> 2674022467520
	2674022467520 [label=AccumulateGrad]
	2674022466272 -> 2674022467088
	2673875215184 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2673875215184 -> 2674022466272
	2674022466272 [label=AccumulateGrad]
	2674022466992 -> 2674022467088
	2673875215344 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2673875215344 -> 2674022466992
	2674022466992 [label=AccumulateGrad]
	2674022467232 -> 2674022467280
	2674022466032 -> 2674022466656
	2673875216544 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2673875216544 -> 2674022466032
	2674022466032 [label=AccumulateGrad]
	2674022466608 -> 2674022466848
	2673875216304 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2673875216304 -> 2674022466608
	2674022466608 [label=AccumulateGrad]
	2674022466896 -> 2674022466848
	2673875216464 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2673875216464 -> 2674022466896
	2674022466896 [label=AccumulateGrad]
	2674022466512 -> 2674022466128
	2673875217104 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2673875217104 -> 2674022466512
	2674022466512 [label=AccumulateGrad]
	2674022465888 -> 2674022466176
	2673875217024 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2673875217024 -> 2674022465888
	2674022465888 [label=AccumulateGrad]
	2674022466080 -> 2674022466176
	2673875217184 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2673875217184 -> 2674022466080
	2674022466080 [label=AccumulateGrad]
	2674022466320 -> 2674022466368
	2674022466320 -> 2674022347104 [dir=none]
	2674022347104 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2674022466320 -> 2674022429824 [dir=none]
	2674022429824 [label="result1
 (0)" fillcolor=orange]
	2674022466320 -> 2674022429744 [dir=none]
	2674022429744 [label="result2
 (0)" fillcolor=orange]
	2674022466320 -> 2673875215744 [dir=none]
	2673875215744 [label="running_mean
 (128)" fillcolor=orange]
	2674022466320 -> 2673875216064 [dir=none]
	2673875216064 [label="running_var
 (128)" fillcolor=orange]
	2674022466320 -> 2673875215904 [dir=none]
	2673875215904 [label="weight
 (128)" fillcolor=orange]
	2674022466320 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673980165808 -> 2674022466320
	2673980165808 -> 2674022346624 [dir=none]
	2674022346624 [label="input
 (1, 64, 56, 56)" fillcolor=orange]
	2673980165808 -> 2673875215824 [dir=none]
	2673875215824 [label="weight
 (128, 64, 1, 1)" fillcolor=orange]
	2673980165808 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2674022465792 -> 2673980165808
	2674022467136 -> 2673980165808
	2673875215824 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2673875215824 -> 2674022467136
	2674022467136 [label=AccumulateGrad]
	2674022466704 -> 2674022466320
	2673875215904 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2673875215904 -> 2674022466704
	2674022466704 [label=AccumulateGrad]
	2674022466464 -> 2674022466320
	2673875215984 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2673875215984 -> 2674022466464
	2674022466464 [label=AccumulateGrad]
	2674022465696 -> 2673980178048
	2673875217744 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2673875217744 -> 2674022465696
	2674022465696 [label=AccumulateGrad]
	2673980177568 -> 2673980178240
	2673875217664 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2673875217664 -> 2673980177568
	2673980177568 [label=AccumulateGrad]
	2674022465840 -> 2673980178240
	2673875217824 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2673875217824 -> 2674022465840
	2674022465840 [label=AccumulateGrad]
	2673980178384 -> 2673980177904
	2673875218384 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2673875218384 -> 2673980178384
	2673980178384 [label=AccumulateGrad]
	2673980172672 -> 2673980177952
	2673875218304 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2673875218304 -> 2673980172672
	2673980172672 [label=AccumulateGrad]
	2673980177856 -> 2673980177952
	2673875218464 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2673875218464 -> 2673980177856
	2673980177856 [label=AccumulateGrad]
	2673980178096 -> 2673980178144
	2673980175840 -> 2673980174304
	2673875219664 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2673875219664 -> 2673980175840
	2673980175840 [label=AccumulateGrad]
	2673980172480 -> 2673980177712
	2673875219584 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2673875219584 -> 2673980172480
	2673980172480 [label=AccumulateGrad]
	2673980177760 -> 2673980177712
	2673875219744 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2673875219744 -> 2673980177760
	2673980177760 [label=AccumulateGrad]
	2673980177472 -> 2673980171472
	2673875220304 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2673875220304 -> 2673980177472
	2673980177472 [label=AccumulateGrad]
	2673980165328 -> 2673980172288
	2673875220224 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2673875220224 -> 2673980165328
	2673980165328 [label=AccumulateGrad]
	2673980165088 -> 2673980172288
	2673875220384 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2673875220384 -> 2673980165088
	2673980165088 [label=AccumulateGrad]
	2673980173248 -> 2673980170752
	2673980173248 -> 2674022347824 [dir=none]
	2674022347824 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2673980173248 -> 2674022515888 [dir=none]
	2674022515888 [label="result1
 (0)" fillcolor=orange]
	2673980173248 -> 2674022515808 [dir=none]
	2674022515808 [label="result2
 (0)" fillcolor=orange]
	2673980173248 -> 2673875218864 [dir=none]
	2673875218864 [label="running_mean
 (256)" fillcolor=orange]
	2673980173248 -> 2673875219184 [dir=none]
	2673875219184 [label="running_var
 (256)" fillcolor=orange]
	2673980173248 -> 2673875219024 [dir=none]
	2673875219024 [label="weight
 (256)" fillcolor=orange]
	2673980173248 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673980177376 -> 2673980173248
	2673980177376 -> 2674022347344 [dir=none]
	2674022347344 [label="input
 (1, 128, 28, 28)" fillcolor=orange]
	2673980177376 -> 2673875218944 [dir=none]
	2673875218944 [label="weight
 (256, 128, 1, 1)" fillcolor=orange]
	2673980177376 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2673980172624 -> 2673980177376
	2673980178336 -> 2673980177376
	2673875218944 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2673875218944 -> 2673980178336
	2673980178336 [label=AccumulateGrad]
	2673980178000 -> 2673980173248
	2673875219024 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2673875219024 -> 2673980178000
	2673980178000 [label=AccumulateGrad]
	2673980171328 -> 2673980173248
	2673875219104 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2673875219104 -> 2673980171328
	2673980171328 [label=AccumulateGrad]
	2673980165232 -> 2673980162592
	2673875220944 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2673875220944 -> 2673980165232
	2673980165232 [label=AccumulateGrad]
	2673980166048 -> 2673980163024
	2673875220864 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2673875220864 -> 2673980166048
	2673980166048 [label=AccumulateGrad]
	2673980169696 -> 2673980163024
	2673875221024 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2673875221024 -> 2673980169696
	2673980169696 [label=AccumulateGrad]
	2673980165040 -> 2673980169072
	2673875221584 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2673875221584 -> 2673980165040
	2673980165040 [label=AccumulateGrad]
	2673980164176 -> 2673980170272
	2673875221504 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2673875221504 -> 2673980164176
	2673980164176 [label=AccumulateGrad]
	2673980173488 -> 2673980170272
	2673875221664 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2673875221664 -> 2673980173488
	2673980173488 [label=AccumulateGrad]
	2673980165520 -> 2673980169264
	2673980176176 -> 2673980163312
	2673875354000 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2673875354000 -> 2673980176176
	2673980176176 [label=AccumulateGrad]
	2673980162448 -> 2673980168496
	2673875353920 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2673875353920 -> 2673980162448
	2673980162448 [label=AccumulateGrad]
	2673980176752 -> 2673980168496
	2673875354080 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2673875354080 -> 2673980176752
	2673980176752 [label=AccumulateGrad]
	2673980162736 -> 2673927058144
	2673875354640 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2673875354640 -> 2673980162736
	2673980162736 [label=AccumulateGrad]
	2673927059344 -> 2673927050320
	2673875354560 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2673875354560 -> 2673927059344
	2673927059344 [label=AccumulateGrad]
	2673980162640 -> 2673927050320
	2673875354720 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2673875354720 -> 2673980162640
	2673980162640 [label=AccumulateGrad]
	2673927059776 -> 2673927059248
	2673927059776 -> 2674022348544 [dir=none]
	2674022348544 [label="input
 (1, 512, 7, 7)" fillcolor=orange]
	2673927059776 -> 2674022519888 [dir=none]
	2674022519888 [label="result1
 (0)" fillcolor=orange]
	2673927059776 -> 2674022519968 [dir=none]
	2674022519968 [label="result2
 (0)" fillcolor=orange]
	2673927059776 -> 2673875222064 [dir=none]
	2673875222064 [label="running_mean
 (512)" fillcolor=orange]
	2673927059776 -> 2673875222384 [dir=none]
	2673875222384 [label="running_var
 (512)" fillcolor=orange]
	2673927059776 -> 2673875222224 [dir=none]
	2673875222224 [label="weight
 (512)" fillcolor=orange]
	2673927059776 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	2673980164128 -> 2673927059776
	2673980164128 -> 2674022348064 [dir=none]
	2674022348064 [label="input
 (1, 256, 14, 14)" fillcolor=orange]
	2673980164128 -> 2673875222144 [dir=none]
	2673875222144 [label="weight
 (512, 256, 1, 1)" fillcolor=orange]
	2673980164128 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2673980173920 -> 2673980164128
	2673980172336 -> 2673980164128
	2673875222144 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2673875222144 -> 2673980172336
	2673980172336 [label=AccumulateGrad]
	2673980172864 -> 2673927059776
	2673875222224 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2673875222224 -> 2673980172864
	2673980172864 [label=AccumulateGrad]
	2673980176128 -> 2673927059776
	2673875222304 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2673875222304 -> 2673980176128
	2673980176128 [label=AccumulateGrad]
	2673927049360 -> 2673980054528
	2673875355280 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2673875355280 -> 2673927049360
	2673927049360 [label=AccumulateGrad]
	2673927059392 -> 2673980049344
	2673875355200 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2673875355200 -> 2673927059392
	2673927059392 [label=AccumulateGrad]
	2673927050176 -> 2673980049344
	2673875355360 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2673875355360 -> 2673927050176
	2673927050176 [label=AccumulateGrad]
	2673860203120 -> 2673871198704
	2673875355920 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2673875355920 -> 2673860203120
	2673860203120 [label=AccumulateGrad]
	2673871198800 -> 2673875080944
	2673875355840 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2673875355840 -> 2673871198800
	2673871198800 [label=AccumulateGrad]
	2673871199568 -> 2673875080944
	2673875356000 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2673875356000 -> 2673871199568
	2673871199568 [label=AccumulateGrad]
	2673875083008 -> 2673980050688
	2673980058848 -> 2673980053760
	2673980058848 [label=TBackward0]
	2673875084832 -> 2673980058848
	2673875500832 [label="fc.weight
 (6, 512)" fillcolor=lightblue]
	2673875500832 -> 2673875084832
	2673875084832 [label=AccumulateGrad]
	2673980053760 -> 2674022348864
}
